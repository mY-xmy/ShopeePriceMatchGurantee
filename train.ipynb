{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.modules import CrossEntropyLoss\n",
    "\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from utils import ShopeeTrainDataset, ShopeeImageDataset, DistancePredict, get_metric, ShopeeScheduler\n",
    "\n",
    "import timm\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    PATH = \"./shopee-product-matching/\"\n",
    "    \n",
    "    epoch = 15\n",
    "    batch_size = 16\n",
    "    num_workers=8\n",
    "    prefetch_factor =8\n",
    "    report_every_batch = 50\n",
    "    \n",
    "    scheduler_params = {\n",
    "        \"lr_start\": 1e-5,\n",
    "        \"lr_max\": 1e-5 * batch_size,     # 1e-5 * 32 (if batch_size(=32) is different then)\n",
    "        \"lr_min\": 1e-6,\n",
    "        \"lr_ramp_ep\": 5,\n",
    "        \"lr_sus_ep\": 2,\n",
    "        \"lr_decay\": 0.8,\n",
    "    }\n",
    "    arcface_scheduler_params = {\n",
    "        \"lr_start\": 1e-4,\n",
    "        \"lr_max\": 1e-4 * batch_size,     # 1e-5 * 32 (if batch_size(=32) is different then)\n",
    "        \"lr_min\": 1e-5,\n",
    "        \"lr_ramp_ep\": 5,\n",
    "        \"lr_sus_ep\": 2,\n",
    "        \"lr_decay\": 0.8,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFace(nn.Module):\n",
    "    \"\"\" NN module for projecting extracted embeddings onto the sphere surface \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, s=30, m=0.5):\n",
    "        super(ArcFace, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.cos_m = math.cos(self.m)\n",
    "        self.sin_m = math.sin(self.m)\n",
    "        self.arc_min = math.cos(math.pi - self.m)\n",
    "        self.margin_min = math.sin(math.pi - self.m) * self.m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "    \n",
    "    def _update_margin(self, new_margin):\n",
    "        self.m = new_margin\n",
    "        self.cos_m = math.cos(self.m)\n",
    "        self.sin_m = math.sin(self.m)\n",
    "        self.arc_min = math.cos(math.pi - self.m)\n",
    "        self.margin_min = math.sin(math.pi - self.m) * self.m\n",
    "\n",
    "    def forward(self, embedding, label):\n",
    "        cos = F.linear(F.normalize(embedding), F.normalize(self.weight))\n",
    "        sin = torch.sqrt(1.0 - torch.pow(cos, 2)).clamp(0, 1)\n",
    "        phi = cos * self.cos_m - sin * self.sin_m\n",
    "        phi = torch.where(cos > self.arc_min, phi, cos - self.margin_min)\n",
    "\n",
    "        one_hot = torch.zeros(cos.size(), device=device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        logits = one_hot * phi + (1.0 - one_hot) * cos\n",
    "        logits *= self.s\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name, n_classes, margin=0.5, fc_dim=1024):\n",
    "        super(Model, self).__init__()\n",
    "        print(\"Building Model Backbone for {} model\".format(model_name))\n",
    "        self.model_name = model_name\n",
    "        self.backbone = timm.create_model(model_name, pretrained=True)\n",
    "        \n",
    "        if \"eca_nfnet\" in model_name:\n",
    "            feat_size = self.backbone.head.fc.in_features\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "                \n",
    "        elif \"efficientnet\" in model_name:\n",
    "            feat_size = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "        \n",
    "        self.pooling =  nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.fc = nn.Linear(feat_size, fc_dim)\n",
    "        self.bn = nn.BatchNorm1d(fc_dim)\n",
    "        self.margin = ArcFace(fc_dim, n_classes, m=margin)\n",
    "        self._init_params()\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = self.pooling(x).view(batch_size, -1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.normalize(x,dim=1)\n",
    "        if labels is not None:\n",
    "            return self.margin(x,labels)\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(name=\"train\"):\n",
    "    assert name in {\"train\", \"test\"}\n",
    "    df = pd.read_csv(config.PATH + '{}.csv'.format(name))\n",
    "    df[\"image_path\"] = config.PATH + '{}_images/'.format(name) + df['image']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>image_path</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>./shopee-product-matching/train_images/0000a68...</td>\n",
       "      <td>[train_129225211, train_2278313361]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>./shopee-product-matching/train_images/0003978...</td>\n",
       "      <td>[train_3386243561, train_3423213080]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>./shopee-product-matching/train_images/000a190...</td>\n",
       "      <td>[train_2288590299, train_3803689425]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>./shopee-product-matching/train_images/00117e4...</td>\n",
       "      <td>[train_2406599165, train_3342059966]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>./shopee-product-matching/train_images/00136d1...</td>\n",
       "      <td>[train_3369186413, train_921438619]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                          Paper Bag Victoria Secret    249114794   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n",
       "4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n",
       "\n",
       "                                          image_path  \\\n",
       "0  ./shopee-product-matching/train_images/0000a68...   \n",
       "1  ./shopee-product-matching/train_images/0003978...   \n",
       "2  ./shopee-product-matching/train_images/000a190...   \n",
       "3  ./shopee-product-matching/train_images/00117e4...   \n",
       "4  ./shopee-product-matching/train_images/00136d1...   \n",
       "\n",
       "                                 target  \n",
       "0   [train_129225211, train_2278313361]  \n",
       "1  [train_3386243561, train_3423213080]  \n",
       "2  [train_2288590299, train_3803689425]  \n",
       "3  [train_2406599165, train_3342059966]  \n",
       "4   [train_3369186413, train_921438619]  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = read_dataset(\"train\")\n",
    "label_group_dict = train.groupby(\"label_group\").posting_id.agg(\"unique\").to_dict()\n",
    "train['target'] = train.label_group.map(label_group_dict)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28194 6056\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(train[\"label_group\"].unique())\n",
    "num = int(0.2 * n_classes)\n",
    "np.random.seed(1)\n",
    "test_group = np.random.choice(train[\"label_group\"].unique(), num)\n",
    "#test_group\n",
    "df_test = train[train[\"label_group\"].isin(test_group)]\n",
    "df_train = train[~train[\"label_group\"].isin(test_group)]\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Repository\\Kaggle\\Shopee\\utils.py:29: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.df['label_class'] = self.df['label_group'].map(class_mapping)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "train_dataset = ShopeeTrainDataset(df_train, transform = transform)\n",
    "test_dataset = ShopeeTrainDataset(df_test, transform = transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9024"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = len(train_dataset.df['label_group'].unique())\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feature(model, dataloader):\n",
    "    image_features = []\n",
    "    with torch.no_grad():\n",
    "        for (images, labels) in tqdm(dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            features = model(images)\n",
    "            image_features.append(features)\n",
    "            del images, labels\n",
    "    image_features = torch.cat(image_features, axis=0)\n",
    "\n",
    "    torch.cuda.empty_cache()   \n",
    "    return image_features\n",
    "\n",
    "def validate(feature, threshold, df):\n",
    "    pred = DistancePredict(df, feature, threshold= threshold)\n",
    "    df[\"pred\"] = pred\n",
    "    f1, prec, rec = get_metric(df[\"target\"], df[\"pred\"])\n",
    "    return f1, prec, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def train(model_name, max_epochs, threshold, margin_min, margin_max):\n",
    "    if not os.path.exists(\"model/{model_name}\".format(model_name=model_name)):\n",
    "        os.makedirs(\"model/{model_name}\".format(model_name=model_name))\n",
    "        \n",
    "    best_f1 = -float(\"inf\")\n",
    "    \n",
    "    model = Model(model_name, n_classes=n_classes, margin=margin_min).to(device)\n",
    "    backbone_params = model.backbone.parameters()\n",
    "    backbone_params_id = list(map(id, model.backbone.parameters()))\n",
    "    other_params = filter(lambda p: id(p) not in backbone_params_id, model.parameters())\n",
    "\n",
    "    optimizer = optim.AdamW(backbone_params, lr=config.scheduler_params['lr_start'])\n",
    "    scheduler = scheduler = ShopeeScheduler(optimizer, **config.scheduler_params)\n",
    "    optimizer2 = optim.AdamW(other_params, lr=config.arcface_scheduler_params['lr_start'])\n",
    "    scheduler2 = scheduler = ShopeeScheduler(optimizer2, **config.arcface_scheduler_params)\n",
    "\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(1,max_epochs+1):\n",
    "        margin = (margin_max - margin_min) / max_epochs * (epoch - 1) + margin_min\n",
    "        model.margin._update_margin(margin)\n",
    "        print(\"Epoch: {:4f}  margin: {:2f}\".format(epoch, margin))\n",
    "        # Train\n",
    "        model.train()\n",
    "        for batch_id, (images, labels) in enumerate(train_dataloader):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model(images, labels)\n",
    "            loss = criterion(output, labels)\n",
    "            optimizer.zero_grad()\n",
    "            optimizer2.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer2.step()\n",
    "\n",
    "            if batch_id % config.report_every_batch == 0:\n",
    "                # Calculate the accuracy on the batch\n",
    "                output = output.data.cpu().numpy()\n",
    "                output = np.argmax(output, axis=1)\n",
    "                labels = labels.data.cpu().numpy()\n",
    "                accuracy = accuracy_score(labels, output)\n",
    "                F1 = f1_score(labels, output, labels=list(set(labels).union(set(output))), average='macro')\n",
    "                # Print info\n",
    "                print('Epoch: {:4}   [ {:5}/{}  ({:3.0f}%) ] \\t\\t Train Loss: {:.6f}     Accuracy: {:.3f}     F1 Score: {:.3f}'.format(\n",
    "                    epoch,\n",
    "                    batch_id * len(images), \n",
    "                    len(train_dataset), \n",
    "                    100. * batch_id / len(train_dataloader), \n",
    "                    loss.data,\n",
    "                    accuracy,\n",
    "                    F1,)\n",
    "                )\n",
    "            del images, labels, output, loss\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        scheduler.step()\n",
    "        scheduler2.step()\n",
    "\n",
    "        # Test and Save the model\n",
    "        print(\"-----Epoch: {} Validation-----\".format(epoch))\n",
    "        # Test\n",
    "        model.eval()\n",
    "        image_features = get_image_feature(model, test_dataloader)\n",
    "        f1, prec, rec = validate(image_features, threshold, df_test)\n",
    "        if f1 > best_f1:\n",
    "            torch.save(model.state_dict(), \"model/{model_name}/{model_name}_margin_{margin}.pt\".format(model_name = model_name, margin=margin))\n",
    "\n",
    "        del image_features\n",
    "        torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "margin_min = 0.2\n",
    "margin_max = 0.95\n",
    "train(model_name = \"efficientnet_b4\", max_epochs = config.epoch, threshold = 0.7, margin_min = margin_min, margin_max = margin_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "076100c574664d70f70738ca94044dd7fcbdc69611a4f77ee18c0ff1c60955f8"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('xmy': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
