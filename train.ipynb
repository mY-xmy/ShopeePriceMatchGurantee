{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn.modules import CrossEntropyLoss\n",
    "\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from utils import ShopeeTrainDataset, ShopeeImageDataset, DistancePredict, get_metric, ShopeeScheduler, validate, NDCG\n",
    "\n",
    "import timm\n",
    "\n",
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else 'cpu')\n",
    "torch.cuda.set_device(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    PATH = \"./data/\"\n",
    "\n",
    "    model_name = \"eca_nfnet_l0\"\n",
    "\n",
    "    threshold = 0.7\n",
    "    \n",
    "    epoch = 15\n",
    "    batch_size = 16\n",
    "    num_workers=8\n",
    "    prefetch_factor =8\n",
    "    report_every_batch = 20\n",
    "    \n",
    "    backbone_lr = 1e-4\n",
    "    arcface_lr = 1e-2\n",
    "    lr = 1e-3\n",
    "\n",
    "    gamma = 0.1\n",
    "    step_size = 5\n",
    "\n",
    "    margin=0.3\n",
    "    margin_set = None\n",
    "    \n",
    "    backbone_scheduler_params = {\n",
    "        \"lr_start\": 1e-5,\n",
    "        \"lr_max\": 1e-4,     # 1e-5 * 32 (if batch_size(=32) is different then)\n",
    "        \"lr_min\": 1e-6,\n",
    "        \"lr_warmup_ep\": 4,\n",
    "        \"lr_sus_ep\": 0,\n",
    "        \"lr_decay\": 0.8,\n",
    "        \"step_size\": 1,\n",
    "    }\n",
    "    scheduler_params = {\n",
    "        \"lr_start\": 1e-4,\n",
    "        \"lr_max\": 1e-3,     # 1e-5 * 32 (if batch_size(=32) is different then)\n",
    "        \"lr_min\": 1e-5,\n",
    "        \"lr_warmup_ep\": 4,\n",
    "        \"lr_sus_ep\": 0,\n",
    "        \"lr_decay\": 0.8,\n",
    "        \"step_size\": 1,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.DEBUG)\n",
    "# 建立一个filehandler来把日志记录在文件里，级别为debug以上\n",
    "fh = logging.FileHandler(\"log/train_{}.log\".format(config.model_name))\n",
    "fh.setLevel(logging.DEBUG)\n",
    "# 建立一个streamhandler来把日志打在CMD窗口上，级别为error以上\n",
    "ch = logging.StreamHandler()\n",
    "ch.setLevel(logging.INFO)\n",
    "# 设置日志格式\n",
    "formatter = logging.Formatter(\"%(asctime)s - %(name)s - %(levelname)s - %(lineno)s %(message)s\",datefmt=\"%Y-%m-%d %H:%M:%S\")\n",
    "ch.setFormatter(formatter)\n",
    "fh.setFormatter(formatter)\n",
    "#将相应的handler添加在logger对象中\n",
    "logger.addHandler(ch)\n",
    "logger.addHandler(fh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArcFace(nn.Module):\n",
    "    \"\"\" NN module for projecting extracted embeddings onto the sphere surface \"\"\"\n",
    "    \n",
    "    def __init__(self, in_features, out_features, s=30, m=0.5):\n",
    "        super(ArcFace, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.cos_m = math.cos(self.m)\n",
    "        self.sin_m = math.sin(self.m)\n",
    "        self.arc_min = math.cos(math.pi - self.m)\n",
    "        self.margin_min = math.sin(math.pi - self.m) * self.m\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(out_features, in_features))\n",
    "        nn.init.xavier_uniform_(self.weight)\n",
    "    \n",
    "    def _update_margin(self, new_margin):\n",
    "        self.m = new_margin\n",
    "        self.cos_m = math.cos(self.m)\n",
    "        self.sin_m = math.sin(self.m)\n",
    "        self.arc_min = math.cos(math.pi - self.m)\n",
    "        self.margin_min = math.sin(math.pi - self.m) * self.m\n",
    "\n",
    "    def forward(self, embedding, label):\n",
    "        cos = F.linear(F.normalize(embedding), F.normalize(self.weight))\n",
    "        sin = torch.sqrt(1.0 - torch.pow(cos, 2)).clamp(0, 1)\n",
    "        phi = cos * self.cos_m - sin * self.sin_m\n",
    "        phi = torch.where(cos > self.arc_min, phi, cos - self.margin_min)\n",
    "\n",
    "        one_hot = torch.zeros(cos.size(), device=device)\n",
    "        one_hot.scatter_(1, label.view(-1, 1).long(), 1)\n",
    "        logits = one_hot * phi + (1.0 - one_hot) * cos\n",
    "        logits *= self.s\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, model_name, n_classes, margin=0.5, fc_dim=1024):\n",
    "        super(Model, self).__init__()\n",
    "        logger.info(\"Building Model Backbone for {} model\".format(model_name))\n",
    "        self.model_name = model_name\n",
    "        self.backbone = timm.create_model(model_name, pretrained=True)\n",
    "        \n",
    "        if \"eca_nfnet\" in model_name:\n",
    "            feat_size = self.backbone.head.fc.in_features\n",
    "            self.backbone.head.fc = nn.Identity()\n",
    "            self.backbone.head.global_pool = nn.Identity()\n",
    "                \n",
    "        elif \"efficientnet\" in model_name:\n",
    "            feat_size = self.backbone.classifier.in_features\n",
    "            self.backbone.classifier = nn.Identity()\n",
    "            self.backbone.global_pool = nn.Identity()\n",
    "        \n",
    "        self.pooling =  nn.AdaptiveAvgPool2d(1)\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "        self.fc = nn.Linear(feat_size, fc_dim)\n",
    "        self.bn = nn.BatchNorm1d(fc_dim)\n",
    "        self.margin = ArcFace(fc_dim, n_classes, m=margin)\n",
    "        self._init_params()\n",
    "\n",
    "    def _init_params(self):\n",
    "        nn.init.xavier_normal_(self.fc.weight)\n",
    "        nn.init.constant_(self.fc.bias, 0)\n",
    "        nn.init.constant_(self.bn.weight, 1)\n",
    "        nn.init.constant_(self.bn.bias, 0)\n",
    "\n",
    "    def forward(self, x, labels=None):\n",
    "        batch_size = x.shape[0]\n",
    "        x = self.backbone(x)\n",
    "        x = self.pooling(x).view(batch_size, -1)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        x = self.bn(x)\n",
    "        x = F.normalize(x,dim=1)\n",
    "        if labels is not None:\n",
    "            return self.margin(x,labels)\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_dataset(name=\"train\"):\n",
    "    assert name in {\"train\", \"test\"}\n",
    "    df = pd.read_csv(config.PATH + '{}.csv'.format(name))\n",
    "    df[\"image_path\"] = config.PATH + '{}_images/'.format(name) + df['image']\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>posting_id</th>\n",
       "      <th>image</th>\n",
       "      <th>image_phash</th>\n",
       "      <th>title</th>\n",
       "      <th>label_group</th>\n",
       "      <th>image_path</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_129225211</td>\n",
       "      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n",
       "      <td>94974f937d4c2433</td>\n",
       "      <td>Paper Bag Victoria Secret</td>\n",
       "      <td>249114794</td>\n",
       "      <td>./data/train_images/0000a68812bc7e98c42888dfb1...</td>\n",
       "      <td>[train_129225211, train_2278313361]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_3386243561</td>\n",
       "      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n",
       "      <td>af3f9460c2838f0f</td>\n",
       "      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n",
       "      <td>2937985045</td>\n",
       "      <td>./data/train_images/00039780dfc94d01db8676fe78...</td>\n",
       "      <td>[train_3386243561, train_3423213080]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_2288590299</td>\n",
       "      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n",
       "      <td>b94cb00ed3e50f78</td>\n",
       "      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n",
       "      <td>2395904891</td>\n",
       "      <td>./data/train_images/000a190fdd715a2a36faed16e2...</td>\n",
       "      <td>[train_2288590299, train_3803689425]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_2406599165</td>\n",
       "      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n",
       "      <td>8514fc58eafea283</td>\n",
       "      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n",
       "      <td>4093212188</td>\n",
       "      <td>./data/train_images/00117e4fc239b1b641ff08340b...</td>\n",
       "      <td>[train_2406599165, train_3342059966]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_3369186413</td>\n",
       "      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n",
       "      <td>a6f319f924ad708c</td>\n",
       "      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n",
       "      <td>3648931069</td>\n",
       "      <td>./data/train_images/00136d1cf4edede0203f32f05f...</td>\n",
       "      <td>[train_3369186413, train_921438619]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         posting_id                                 image       image_phash  \\\n",
       "0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n",
       "1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n",
       "2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n",
       "3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n",
       "4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n",
       "\n",
       "                                               title  label_group  \\\n",
       "0                          Paper Bag Victoria Secret    249114794   \n",
       "1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n",
       "2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n",
       "3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n",
       "4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n",
       "\n",
       "                                          image_path  \\\n",
       "0  ./data/train_images/0000a68812bc7e98c42888dfb1...   \n",
       "1  ./data/train_images/00039780dfc94d01db8676fe78...   \n",
       "2  ./data/train_images/000a190fdd715a2a36faed16e2...   \n",
       "3  ./data/train_images/00117e4fc239b1b641ff08340b...   \n",
       "4  ./data/train_images/00136d1cf4edede0203f32f05f...   \n",
       "\n",
       "                                 target  \n",
       "0   [train_129225211, train_2278313361]  \n",
       "1  [train_3386243561, train_3423213080]  \n",
       "2  [train_2288590299, train_3803689425]  \n",
       "3  [train_2406599165, train_3342059966]  \n",
       "4   [train_3369186413, train_921438619]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = read_dataset(\"train\")\n",
    "label_group_dict = train.groupby(\"label_group\").posting_id.agg(\"unique\").to_dict()\n",
    "train['target'] = train.label_group.map(label_group_dict)\n",
    "train.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28194 6056\n"
     ]
    }
   ],
   "source": [
    "n_classes = len(train[\"label_group\"].unique())\n",
    "num = int(0.2 * n_classes)\n",
    "np.random.seed(1)\n",
    "test_group = np.random.choice(train[\"label_group\"].unique(), num)\n",
    "#test_group\n",
    "df_test = train[train[\"label_group\"].isin(test_group)]\n",
    "df_train = train[~train[\"label_group\"].isin(test_group)]\n",
    "df_test.reset_index(drop=True, inplace=True)\n",
    "print(len(df_train), len(df_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yxfang/xmy/shopee/utils.py:28: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.df['label_class'] = self.df['label_group'].map(class_mapping)\n"
     ]
    }
   ],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "train_dataset = ShopeeTrainDataset(df_train, transform = transform)\n",
    "test_dataset = ShopeeImageDataset(df_test, transform = transform)\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, num_workers=2)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9024"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = len(train_dataset.df['label_group'].unique())\n",
    "n_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_feature(model, dataloader):\n",
    "    image_features = []\n",
    "    with torch.no_grad():\n",
    "        for images in tqdm(dataloader):\n",
    "            images = images.to(device)\n",
    "            features = model(images)\n",
    "            image_features.append(features)\n",
    "            del images\n",
    "    image_features = torch.cat(image_features, axis=0)\n",
    "\n",
    "    torch.cuda.empty_cache()   \n",
    "    return image_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(data_loader, model, criterion, optimizers, schedulers):\n",
    "    model.train()\n",
    "    fin_loss = 0.0\n",
    "    \n",
    "    for batch_id, (images, labels) in enumerate(tqdm(data_loader)):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            output = model(images, labels)\n",
    "            loss = criterion(output, labels)\n",
    "            for opt in optimizers:\n",
    "                opt.zero_grad()\n",
    "            loss.backward()\n",
    "            for opt in optimizers:\n",
    "                opt.step()\n",
    "            fin_loss += loss.item() \n",
    "\n",
    "            if batch_id % config.report_every_batch == 0:\n",
    "                logger.debug(\"Batch: {}/{}  loss: {:4f}\".format((batch_id+1), len(data_loader), loss.item()))\n",
    "        \n",
    "    for scheduler in schedulers:\n",
    "        scheduler.step()\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    return fin_loss / len(data_loader)\n",
    "\n",
    "def eval_fn(data_loader, model, df):\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        image_features = get_image_feature(model, data_loader)\n",
    "        ndcg = NDCG(image_features, df)\n",
    "\n",
    "        del image_features\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        return ndcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(model_name, max_epochs, threshold, margin, margin_set = None):\n",
    "    if not os.path.exists(\"model/{model_name}\".format(model_name=model_name)):\n",
    "        os.makedirs(\"model/{model_name}\".format(model_name=model_name))\n",
    "    \n",
    "    model = Model(model_name, n_classes=n_classes, margin=margin).to(device)\n",
    "    backbone_params = model.backbone.parameters()\n",
    "    backbone_params_id = list(map(id, model.backbone.parameters()))\n",
    "    arcface_params = model.margin.parameters()\n",
    "    arcface_params_id = list(map(id, model.margin.parameters()))\n",
    "    other_params = filter(lambda p: id(p) not in backbone_params_id and id(p) not in arcface_params_id, model.parameters())\n",
    "\n",
    "    params = [\n",
    "        {\"params\": backbone_params, \"lr\": config.backbone_lr},\n",
    "        {\"params\": arcface_params, \"lr\": config.arcface_lr},\n",
    "        {\"params\": other_params}\n",
    "    ]\n",
    "\n",
    "    optimizer = optim.AdamW(params, lr=config.lr)\n",
    "    scheduler  = optim.lr_scheduler.StepLR(optimizer, step_size=config.step_size, gamma=config.gamma)\n",
    "\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        # update margin\n",
    "        if margin_set is not None and epoch in margin_set:\n",
    "            margin = margin_set[epoch]\n",
    "            model.margin._update_margin(margin)\n",
    "            logger.info(\"Epoch: {}  margin: {:2f}\".format(epoch, margin))\n",
    "        # Train\n",
    "        logger.info(\"-----Epoch: {} Train-----\".format(epoch))\n",
    "        train_avg_loss = train_fn(train_dataloader, model, criterion, [optimizer], [scheduler])\n",
    "        logger.info(\"Epoch: {}  avg loss: {:4f}\".format(epoch, train_avg_loss))\n",
    "\n",
    "        # eval\n",
    "        logger.info(\"-----Epoch: {} Validation-----\".format(epoch))\n",
    "        ndcg = eval_fn(test_dataloader, model, df_test)\n",
    "        logger.info(\"Epoch: {}  NDCG: {:4f}\".format(epoch, ndcg.mean()))\n",
    "        \n",
    "        torch.save(model.state_dict(), \"model/{model_name}/{model_name}_epoch_{epoch}.pt\".format(model_name = model_name, epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-11-09 06:22:45 - root - INFO - 4 Building Model Backbone for eca_nfnet_l0 model\n",
      "2021-11-09 06:22:46 - timm.models.helpers - INFO - 183 Loading pretrained weights from url (https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/ecanfnet_l0_ra2-e3e9ac50.pth)\n",
      "2021-11-09 06:22:48 - root - INFO - 30 -----Epoch: 0 Train-----\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "308de319866148a9ae1aff958437badd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1763 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "run_train(model_name = config.model_name, max_epochs = config.epoch, threshold = config.threshold, margin = config.margin, margin_set=config.margin_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b2b58f3dac0de255a91c15f7cf4de9f1b2a6acaa1fd80edad0aec8656f588f0c"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
